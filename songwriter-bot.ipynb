{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08f547b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:38:21.365185Z",
     "iopub.status.busy": "2023-05-16T10:38:21.364457Z",
     "iopub.status.idle": "2023-05-16T10:43:07.502921Z",
     "shell.execute_reply": "2023-05-16T10:43:07.501811Z"
    },
    "papermill": {
     "duration": 286.152601,
     "end_time": "2023-05-16T10:43:07.507248",
     "exception": false,
     "start_time": "2023-05-16T10:38:21.354647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-nlp\r\n",
      "  Downloading keras_nlp-0.3.1-py3-none-any.whl (151 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.1/151.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from keras-nlp) (22.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from keras-nlp) (1.21.6)\r\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from keras-nlp) (0.15.0)\r\n",
      "Collecting tensorflow-text\r\n",
      "  Downloading tensorflow_text-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (from keras-nlp) (2.6.4)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py->keras-nlp) (1.15.0)\r\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.12)\r\n",
      "Requirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (5.0)\r\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (0.37.1)\r\n",
      "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (2.6.0)\r\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (0.2.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (2.6.0)\r\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.12.1)\r\n",
      "Collecting h5py~=3.1.0\r\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting numpy\r\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (2.6.0)\r\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.1.2)\r\n",
      "Collecting typing-extensions<3.11,>=3.7\r\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.6.3)\r\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.1.0)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (3.20.3)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.51.1)\r\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (3.3.0)\r\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-text->keras-nlp) (0.12.0)\r\n",
      "Collecting tensorflow\r\n",
      "  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting tensorflow-text\r\n",
      "  Downloading tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (1.35.0)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (59.8.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (2.28.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (0.6.1)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (2.2.2)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (1.8.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (3.3.7)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (0.4.6)\r\n",
      "Collecting tensorflow\r\n",
      "  Downloading tensorflow-2.10.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-text\r\n",
      "  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow\r\n",
      "  Downloading tensorflow-2.9.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow-2.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-text\r\n",
      "  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow\r\n",
      "  Downloading tensorflow-2.8.4-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.9/497.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\r\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow\r\n",
      "  Downloading tensorflow-2.8.3-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.9/497.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow-2.8.2-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.9/497.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow-2.8.1-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.9/497.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.5/497.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-text\r\n",
      "  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow\r\n",
      "  Downloading tensorflow-2.7.4-cp37-cp37m-manylinux2010_x86_64.whl (495.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.5/495.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (495.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.4/495.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow-2.7.2-cp37-cp37m-manylinux2010_x86_64.whl (495.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.4/495.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow-2.7.1-cp37-cp37m-manylinux2010_x86_64.whl (495.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.0/495.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.6/489.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-text\r\n",
      "  Downloading tensorflow_text-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow->keras-nlp) (1.5.2)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (4.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (4.13.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (1.26.13)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (2022.12.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (3.8.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (3.2.0)\r\n",
      "Installing collected packages: typing-extensions, numpy, h5py, tensorflow-text, keras-nlp\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.1.1\r\n",
      "    Uninstalling typing_extensions-4.1.1:\r\n",
      "      Successfully uninstalled typing_extensions-4.1.1\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.21.6\r\n",
      "    Uninstalling numpy-1.21.6:\r\n",
      "      Successfully uninstalled numpy-1.21.6\r\n",
      "  Attempting uninstall: h5py\r\n",
      "    Found existing installation: h5py 3.7.0\r\n",
      "    Uninstalling h5py-3.7.0:\r\n",
      "      Successfully uninstalled h5py-3.7.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\r\n",
      "dask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\r\n",
      "beatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\r\n",
      "tfx-bsl 1.9.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.52.0 which is incompatible.\r\n",
      "tfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "tensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "tensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "rich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "pytorch-lightning 1.8.6 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "pytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "pytoolconfig 1.2.4 requires typing-extensions>=4.4.0; python_version < \"3.8\", but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "pdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\r\n",
      "pandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\r\n",
      "ortools 9.5.2237 requires protobuf>=4.21.5, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "nnabla 1.32.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\r\n",
      "nnabla 1.32.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\r\n",
      "jaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\r\n",
      "jax 0.3.25 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\r\n",
      "imbalanced-learn 0.10.1 requires joblib>=1.1.1, but you have joblib 1.0.1 which is incompatible.\r\n",
      "flax 0.6.3 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "flake8 5.0.4 requires importlib-metadata<4.3,>=1.1.0; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\r\n",
      "featuretools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\r\n",
      "dask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\r\n",
      "dask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\r\n",
      "cupy-cuda110 11.4.0 requires numpy<1.26,>=1.20, but you have numpy 1.19.5 which is incompatible.\r\n",
      "cmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 4.13.0 which is incompatible.\r\n",
      "cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\r\n",
      "apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\r\n",
      "allennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\r\n",
      "allennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\r\n",
      "aioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "aiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.44 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed h5py-3.1.0 keras-nlp-0.3.1 numpy-1.19.5 tensorflow-text-2.6.0 typing-extensions-3.10.0.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\r\\nA...  \n",
       "1  Take it easy with me, please  \\r\\nTouch me gen...  \n",
       "2  I'll never know why I had to go  \\r\\nWhy I had...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I want to say thank you to the entire python community, without you I would not have learned anything).\n",
    "#This is my first project - the Songwriter telegram bot on NLP. The idea is very simple - the user writes\n",
    "#the first few words and Songwriter continues! Spotify-million-song-dataset was taken for training.\n",
    "\n",
    "import os, sys\n",
    "#These steps are only needed if you are deploying the bot to a server and you need to specify the path to the libraries. \n",
    "#Also don't forget to install pyTelegramBotAPI in your server's terminal.\n",
    "#pip3 install pyTelegramBotAPI \n",
    "#sys.path.append('/usr/local/lib/python3.10/dist-packages/tensorflow')  \n",
    "#sys.path.append('/usr/local/lib/python3.10/dist-packages/keras_nlp')\n",
    "#sys.path.append('/usr/local/lib/python3.10/dist-packages/numpy')\n",
    "#sys.path.append('/usr/local/lib/python3.10/dist-packages/pandas')\n",
    "#sys.path.append('/usr/local/lib/python3.10/dist-packages/pandas/telebot')\n",
    "\n",
    "!pip install keras-nlp --upgrade #Kaggle does not have keras_nlp installed by default.\n",
    "import tensorflow as tf #For building ML and AI models.\n",
    "from tensorflow import keras #To customize your model.\n",
    "from tensorflow.keras.layers import TextVectorization #To create a vector from text.\n",
    "import keras_nlp\n",
    "#import telebot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "data = pd.read_csv('/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv')\n",
    "#If you deployed your bot on a server specify the path to the dataset in a similar way.\n",
    "#data = pd.read_csv('/home/your_bot_name/spotify_millsongdata.csv') \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f407a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:08.038720Z",
     "iopub.status.busy": "2023-05-16T10:43:08.037972Z",
     "iopub.status.idle": "2023-05-16T10:43:08.104925Z",
     "shell.execute_reply": "2023-05-16T10:43:08.103749Z"
    },
    "papermill": {
     "duration": 0.337528,
     "end_time": "2023-05-16T10:43:08.107588",
     "exception": false,
     "start_time": "2023-05-16T10:43:07.770060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Donna Summer        191\n",
       "Gordon Lightfoot    189\n",
       "Bob Dylan           188\n",
       "George Strait       188\n",
       "Alabama             187\n",
       "Cher                187\n",
       "Loretta Lynn        187\n",
       "Reba Mcentire       187\n",
       "Name: artist, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['link'], axis=1)\n",
    "data = data.groupby(\"artist\").filter(lambda x: len(x) > 186)\n",
    "#I take only a small part of the dataset, because it is technically limited.\n",
    "#To take performers who have more than 20 songs would require 47 gigs of RAM!\n",
    "data.artist.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d02603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:08.732759Z",
     "iopub.status.busy": "2023-05-16T10:43:08.732384Z",
     "iopub.status.idle": "2023-05-16T10:43:08.740287Z",
     "shell.execute_reply": "2023-05-16T10:43:08.739231Z"
    },
    "papermill": {
     "duration": 0.326224,
     "end_time": "2023-05-16T10:43:08.743083",
     "exception": false,
     "start_time": "2023-05-16T10:43:08.416859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'Bob Dylan', 'Cher', 'Donna Summer', 'George Strait',\n",
       "       'Gordon Lightfoot', 'Loretta Lynn', 'Reba Mcentire'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.artist.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc9e137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:09.258760Z",
     "iopub.status.busy": "2023-05-16T10:43:09.258381Z",
     "iopub.status.idle": "2023-05-16T10:43:09.269898Z",
     "shell.execute_reply": "2023-05-16T10:43:09.268796Z"
    },
    "papermill": {
     "duration": 0.275563,
     "end_time": "2023-05-16T10:43:09.273706",
     "exception": false,
     "start_time": "2023-05-16T10:43:08.998143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Calling All Angels</td>\n",
       "      <td>Calling, calling all angels, oh I'm calling, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Can't Keep A Good Man Down</td>\n",
       "      <td>I thought it was forever  \\r\\nI thought it wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Carolina Mountain Dew</td>\n",
       "      <td>Somewhere in the mountains......... In norther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Christmas In Dixie</td>\n",
       "      <td>By now in New York City, there's snow on the g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Christmas In Your Arms</td>\n",
       "      <td>All my friends are asking me where I plan to s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      artist                        song  \\\n",
       "361  Alabama          Calling All Angels   \n",
       "362  Alabama  Can't Keep A Good Man Down   \n",
       "363  Alabama       Carolina Mountain Dew   \n",
       "364  Alabama          Christmas In Dixie   \n",
       "365  Alabama      Christmas In Your Arms   \n",
       "\n",
       "                                                  text  \n",
       "361  Calling, calling all angels, oh I'm calling, c...  \n",
       "362  I thought it was forever  \\r\\nI thought it wou...  \n",
       "363  Somewhere in the mountains......... In norther...  \n",
       "364  By now in New York City, there's snow on the g...  \n",
       "365  All my friends are asking me where I plan to s...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0c79f3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:09.786133Z",
     "iopub.status.busy": "2023-05-16T10:43:09.785715Z",
     "iopub.status.idle": "2023-05-16T10:43:09.797557Z",
     "shell.execute_reply": "2023-05-16T10:43:09.796512Z"
    },
    "papermill": {
     "duration": 0.269607,
     "end_time": "2023-05-16T10:43:09.799956",
     "exception": false,
     "start_time": "2023-05-16T10:43:09.530349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Away In A Manger                     3\n",
       "In The Garden                        3\n",
       "I'll Be Home For Christmas           3\n",
       "I Believe                            3\n",
       "All I Really Want To Do              2\n",
       "                                    ..\n",
       "Here I Am Again                      1\n",
       "Help Me Make It Through The Night    1\n",
       "Hello Darlin'                        1\n",
       "Have Mercy On Me                     1\n",
       "One Thin Dime                        1\n",
       "Name: song, Length: 1481, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['artist'], axis=1)\n",
    "data.song.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b993c998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:10.317086Z",
     "iopub.status.busy": "2023-05-16T10:43:10.316658Z",
     "iopub.status.idle": "2023-05-16T10:43:10.391948Z",
     "shell.execute_reply": "2023-05-16T10:43:10.391040Z"
    },
    "papermill": {
     "duration": 0.337489,
     "end_time": "2023-05-16T10:43:10.394349",
     "exception": false,
     "start_time": "2023-05-16T10:43:10.056860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hello = data.to_string() #This step is obligatory, string is needed for further work.\n",
    "guys = hello.replace('\\\\r\\\\n', '').split('.') #Remove garbage from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5a3066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:10.965377Z",
     "iopub.status.busy": "2023-05-16T10:43:10.964943Z",
     "iopub.status.idle": "2023-05-16T10:43:10.971788Z",
     "shell.execute_reply": "2023-05-16T10:43:10.970765Z"
    },
    "papermill": {
     "duration": 0.27007,
     "end_time": "2023-05-16T10:43:10.973918",
     "exception": false,
     "start_time": "2023-05-16T10:43:10.703848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6462469"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b2ac89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:11.488898Z",
     "iopub.status.busy": "2023-05-16T10:43:11.487892Z",
     "iopub.status.idle": "2023-05-16T10:43:11.497205Z",
     "shell.execute_reply": "2023-05-16T10:43:11.496289Z"
    },
    "papermill": {
     "duration": 0.269923,
     "end_time": "2023-05-16T10:43:11.499444",
     "exception": false,
     "start_time": "2023-05-16T10:43:11.229521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_list = list(filter(None, guys))\n",
    "random.shuffle(text_list) #Let's mix our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5aaa6ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:12.100353Z",
     "iopub.status.busy": "2023-05-16T10:43:12.099963Z",
     "iopub.status.idle": "2023-05-16T10:43:12.105365Z",
     "shell.execute_reply": "2023-05-16T10:43:12.104270Z"
    },
    "papermill": {
     "duration": 0.346504,
     "end_time": "2023-05-16T10:43:12.107737",
     "exception": false,
     "start_time": "2023-05-16T10:43:11.761233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "length = len(text_list)\n",
    "text_train = text_list[:int(0.7*length)] #Split it into train, valid.\n",
    "text_valid = text_list[int(0.7*length):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7e53da5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:12.624849Z",
     "iopub.status.busy": "2023-05-16T10:43:12.623807Z",
     "iopub.status.idle": "2023-05-16T10:43:20.790506Z",
     "shell.execute_reply": "2023-05-16T10:43:20.789455Z"
    },
    "papermill": {
     "duration": 8.427434,
     "end_time": "2023-05-16T10:43:20.793160",
     "exception": false,
     "start_time": "2023-05-16T10:43:12.365726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_string):\n",
    "    sentence = tf.strings.lower(input_string)\n",
    "    return sentence\n",
    "\n",
    "maxlen = 50 # You can also set calculate the longest sentence in the data\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = custom_standardization,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(text_list)\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9176be7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:21.307553Z",
     "iopub.status.busy": "2023-05-16T10:43:21.307177Z",
     "iopub.status.idle": "2023-05-16T10:43:21.313757Z",
     "shell.execute_reply": "2023-05-16T10:43:21.312786Z"
    },
    "papermill": {
     "duration": 0.26421,
     "end_time": "2023-05-16T10:43:21.315984",
     "exception": false,
     "start_time": "2023-05-16T10:43:21.051774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14828"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size #The size of our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147c80c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:21.869525Z",
     "iopub.status.busy": "2023-05-16T10:43:21.868837Z",
     "iopub.status.idle": "2023-05-16T10:43:21.879574Z",
     "shell.execute_reply": "2023-05-16T10:43:21.878464Z"
    },
    "papermill": {
     "duration": 0.314082,
     "end_time": "2023-05-16T10:43:21.882105",
     "exception": false,
     "start_time": "2023-05-16T10:43:21.568023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_lookup = dict(zip(range(len(vocab)), vocab))    \n",
    "index_lookup[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7e1811c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:22.403339Z",
     "iopub.status.busy": "2023-05-16T10:43:22.402921Z",
     "iopub.status.idle": "2023-05-16T10:43:22.473979Z",
     "shell.execute_reply": "2023-05-16T10:43:22.473040Z"
    },
    "papermill": {
     "duration": 0.337773,
     "end_time": "2023-05-16T10:43:22.476467",
     "exception": false,
     "start_time": "2023-05-16T10:43:22.138694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64 #You can change this setting.\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(text_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=256)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(text_valid)\n",
    "valid_dataset = valid_dataset.shuffle(buffer_size=256)\n",
    "valid_dataset = valid_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a07d29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:23.078672Z",
     "iopub.status.busy": "2023-05-16T10:43:23.077783Z",
     "iopub.status.idle": "2023-05-16T10:43:23.208673Z",
     "shell.execute_reply": "2023-05-16T10:43:23.207743Z"
    },
    "papermill": {
     "duration": 0.481198,
     "end_time": "2023-05-16T10:43:23.210918",
     "exception": false,
     "start_time": "2023-05-16T10:43:22.729720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_text)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "valid_dataset = valid_dataset.map(preprocess_text)\n",
    "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6897b08b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:23.734766Z",
     "iopub.status.busy": "2023-05-16T10:43:23.734407Z",
     "iopub.status.idle": "2023-05-16T10:43:23.855709Z",
     "shell.execute_reply": "2023-05-16T10:43:23.853670Z"
    },
    "papermill": {
     "duration": 0.386862,
     "end_time": "2023-05-16T10:43:23.858271",
     "exception": false,
     "start_time": "2023-05-16T10:43:23.471409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(64, 50), dtype=int64, numpy=\n",
      "array([[ 116,  147,   40, ...,    0,    0,    0],\n",
      "       [ 137,   65,    0, ...,    0,    0,    0],\n",
      "       [  73,  484,   77, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [  49,   16,   10, ...,    0,    0,    0],\n",
      "       [ 238,   44,   15, ..., 1791,    8,    2],\n",
      "       [  61,  173,  439, ...,    0,    0,    0]])>, <tf.Tensor: shape=(64, 50), dtype=int64, numpy=\n",
      "array([[ 147,   40,    5, ...,    0,    0,    0],\n",
      "       [  65,    0,    0, ...,    0,    0,    0],\n",
      "       [ 484,   77,  339, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [  16,   10, 6552, ...,    0,    0,    0],\n",
      "       [  44,   15,   21, ...,    8,    2,  284],\n",
      "       [ 173,  439,  140, ...,    0,    0,    0]])>)\n"
     ]
    }
   ],
   "source": [
    "for entry in train_dataset.take(1):\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ac02f17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:24.372031Z",
     "iopub.status.busy": "2023-05-16T10:43:24.370943Z",
     "iopub.status.idle": "2023-05-16T10:43:25.563966Z",
     "shell.execute_reply": "2023-05-16T10:43:25.562017Z"
    },
    "papermill": {
     "duration": 1.450377,
     "end_time": "2023-05-16T10:43:25.566618",
     "exception": false,
     "start_time": "2023-05-16T10:43:24.116241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 50, 128)           1904384   \n",
      "_________________________________________________________________\n",
      "transformer_decoder (Transfo (None, 50, 128)           99584     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50, 14828)         1912812   \n",
      "=================================================================\n",
      "Total params: 3,916,780\n",
      "Trainable params: 3,916,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "\n",
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(vocab_size, maxlen, embed_dim)(inputs)\n",
    "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
    "                                                            num_heads=num_heads, \n",
    "                                                            dropout=0.5)(embedding_layer)\n",
    "    \n",
    "    outputs = keras.layers.Dense(vocab_size, activation='softmax')(decoder)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"adam\", \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "#This is the model we got, if resources allow - take more parameters.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c09e02a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:26.084947Z",
     "iopub.status.busy": "2023-05-16T10:43:26.084467Z",
     "iopub.status.idle": "2023-05-16T10:43:26.100173Z",
     "shell.execute_reply": "2023-05-16T10:43:26.099050Z"
    },
    "papermill": {
     "duration": 0.273843,
     "end_time": "2023-05-16T10:43:26.102424",
     "exception": false,
     "start_time": "2023-05-16T10:43:25.828581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextSampler(keras.callbacks.Callback):\n",
    "    def __init__(self, start_prompt, max_tokens):\n",
    "        self.start_prompt = start_prompt\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    # Helper method to choose a word from the top K probable words with respect to their probabilities\n",
    "    # in a sequence\n",
    "    def sample_token(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        decoded_sample = self.start_prompt\n",
    "        \n",
    "        for i in range(self.max_tokens-1):\n",
    "            tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\n",
    "            predictions = self.model.predict([tokenized_prompt], verbose=0)\n",
    "            sample_index = len(decoded_sample.strip().split())-1\n",
    "            \n",
    "            sampled_token = self.sample_token(predictions[0][sample_index])\n",
    "            sampled_token = index_lookup[sampled_token]\n",
    "            decoded_sample += \" \" + sampled_token\n",
    "            \n",
    "        print(f\"\\nSample text:\\n{decoded_sample}...\\n\")\n",
    "\n",
    "# First 5 words of a random sentence to be used as a seed\n",
    "random_sentence = ' '.join(random.choice(text_valid).replace('\\r\\n', '').split(' ')[:4])\n",
    "sampler = TextSampler(random_sentence, 30)\n",
    "reducelr = keras.callbacks.ReduceLROnPlateau(patience=10, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e1c1c61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:43:26.608831Z",
     "iopub.status.busy": "2023-05-16T10:43:26.608154Z",
     "iopub.status.idle": "2023-05-16T10:44:35.868701Z",
     "shell.execute_reply": "2023-05-16T10:44:35.866450Z"
    },
    "papermill": {
     "duration": 69.516141,
     "end_time": "2023-05-16T10:44:35.871662",
     "exception": false,
     "start_time": "2023-05-16T10:43:26.355521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "38/38 [==============================] - 7s 84ms/step - loss: 6.9244 - perplexity: 1016.7596 - accuracy: 0.5293 - val_loss: 3.8851 - val_perplexity: 48.6728 - val_accuracy: 0.5820\n",
      "\n",
      "Sample text:\n",
      "  She still you i  to the you i you the to  the i you i you the i you to    you the i to you to...\n",
      "\n",
      "Epoch 2/15\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 3.4325 - perplexity: 30.9531 - accuracy: 0.5928 - val_loss: 3.1376 - val_perplexity: 23.0479 - val_accuracy: 0.5820\n",
      "\n",
      "Sample text:\n",
      "  She still the  i a  a the the  and you  i and a a you and the a i and   you you you  you...\n",
      "\n",
      "Epoch 3/15\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 2.8498 - perplexity: 17.2850 - accuracy: 0.5949 - val_loss: 2.8917 - val_perplexity: 18.0236 - val_accuracy: 0.5886\n",
      "\n",
      "Sample text:\n",
      "  She still to to a a and you i i i you i i a the and you i you a the a i you a i to i the a...\n",
      "\n",
      "Epoch 4/15\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 2.6610 - perplexity: 14.3104 - accuracy: 0.6026 - val_loss: 2.8250 - val_perplexity: 16.8613 - val_accuracy: 0.5955\n",
      "\n",
      "Sample text:\n",
      "  She still a a i i i you and a a the the the you and you the  i the you to you the i the you   i...\n",
      "\n",
      "Epoch 5/15\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 2.5090 - perplexity: 12.2926 - accuracy: 0.6126 - val_loss: 2.7493 - val_perplexity: 15.6324 - val_accuracy: 0.6063\n",
      "\n",
      "Sample text:\n",
      "  She still be a heart of my the and the lovebug and to a eyes i don't be in i got my the way   to you and i know...\n",
      "\n",
      "Epoch 6/15\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 2.3340 - perplexity: 10.3196 - accuracy: 0.6293 - val_loss: 2.6719 - val_perplexity: 14.4672 - val_accuracy: 0.6140\n",
      "\n",
      "Sample text:\n",
      "  She still be my way and they was you can have the little  and you  was the way of a little  heart of you  was a heart...\n",
      "\n",
      "Epoch 7/15\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 2.1516 - perplexity: 8.5986 - accuracy: 0.6454 - val_loss: 2.6072 - val_perplexity: 13.5607 - val_accuracy: 0.6191\n",
      "\n",
      "Sample text:\n",
      "  She still be your eyes of your eyes and the heart   and he said i know you can you don't have to do you  know what you was...\n",
      "\n",
      "Epoch 8/15\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 1.9786 - perplexity: 7.2323 - accuracy: 0.6633 - val_loss: 2.5714 - val_perplexity: 13.0840 - val_accuracy: 0.6234\n",
      "\n",
      "Sample text:\n",
      "  She still be your love you can be a little man    of my heart and they call the man to make you see you don't have the boy,...\n",
      "\n",
      "Epoch 9/15\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 1.8258 - perplexity: 6.2080 - accuracy: 0.6781 - val_loss: 2.5572 - val_perplexity: 12.8993 - val_accuracy: 0.6253\n",
      "\n",
      "Sample text:\n",
      "  She still see the mood, we can see that he can see the world is the sky is all you don't see that he don't let the land of the lights...\n",
      "\n",
      "Epoch 10/15\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 1.6996 - perplexity: 5.4720 - accuracy: 0.6898 - val_loss: 2.5569 - val_perplexity: 12.8962 - val_accuracy: 0.6275\n",
      "\n",
      "Sample text:\n",
      "  She still see the way  up and you know how it to you know that i don't know i got my dreams in love is the one to stay ...\n",
      "\n",
      "Epoch 11/15\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 1.5956 - perplexity: 4.9311 - accuracy: 0.6972 - val_loss: 2.5574 - val_perplexity: 12.9022 - val_accuracy: 0.6280\n",
      "\n",
      "Sample text:\n",
      "  She still can't stand to me, oh my head and you and the other time for the trees i'm not a lot of a little trip to tell me and he...\n",
      "\n",
      "Epoch 12/15\n",
      "38/38 [==============================] - 3s 76ms/step - loss: 1.5087 - perplexity: 4.5208 - accuracy: 0.7022 - val_loss: 2.5752 - val_perplexity: 13.1342 - val_accuracy: 0.6289\n",
      "\n",
      "Sample text:\n",
      "  She still yearnin' some old game i don't you know what you and you can do  you know i know that i've been so hard times in my heart and...\n",
      "\n",
      "Epoch 13/15\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 1.4355 - perplexity: 4.2020 - accuracy: 0.7070 - val_loss: 2.5940 - val_perplexity: 13.3832 - val_accuracy: 0.6300\n",
      "\n",
      "Sample text:\n",
      "  She still yearnin' a fool on a big roar,  uh-huh  outside  of the wind blows hard riding cowboys riding on me to see that is just like you...\n",
      "\n",
      "Epoch 14/15\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 1.3721 - perplexity: 3.9435 - accuracy: 0.7112 - val_loss: 2.6168 - val_perplexity: 13.6918 - val_accuracy: 0.6298\n",
      "\n",
      "Sample text:\n",
      "  She still be there is long, and the hole i can't stand and i'll be  free and they did they call your rocks sky sometimes you  don't fall on...\n",
      "\n",
      "Epoch 15/15\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 1.3180 - perplexity: 3.7359 - accuracy: 0.7161 - val_loss: 2.6403 - val_perplexity: 14.0179 - val_accuracy: 0.6301\n",
      "\n",
      "Sample text:\n",
      "  She still love you can send us with a chill, 'cause the locusts crying, come down the trees i'm in a man who is just like it was a dollar ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "history = model.fit(train_dataset, \n",
    "                    validation_data=valid_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=15, #You can make more epochs, but the text will become boring.\n",
    "                    callbacks=[sampler, reducelr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f2bc50b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:44:36.483029Z",
     "iopub.status.busy": "2023-05-16T10:44:36.481834Z",
     "iopub.status.idle": "2023-05-16T10:44:36.492463Z",
     "shell.execute_reply": "2023-05-16T10:44:36.491263Z"
    },
    "papermill": {
     "duration": 0.314302,
     "end_time": "2023-05-16T10:44:36.494784",
     "exception": false,
     "start_time": "2023-05-16T10:44:36.180482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_token(logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "def generate_text(prompt, response_length=30):\n",
    "    decoded_sample = prompt\n",
    "    for i in range(response_length-1):\n",
    "        tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\n",
    "        predictions = model.predict([tokenized_prompt], verbose=0)\n",
    "        sample_index = len(decoded_sample.strip().split())-1\n",
    "\n",
    "        sampled_token = sample_token(predictions[0][sample_index])\n",
    "        sampled_token = index_lookup[sampled_token]\n",
    "        decoded_sample += \" \" + sampled_token\n",
    "    return decoded_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0347e77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:44:37.091453Z",
     "iopub.status.busy": "2023-05-16T10:44:37.091049Z",
     "iopub.status.idle": "2023-05-16T10:44:38.498474Z",
     "shell.execute_reply": "2023-05-16T10:44:38.497352Z"
    },
    "papermill": {
     "duration": 1.707669,
     "end_time": "2023-05-16T10:44:38.501266",
     "exception": false,
     "start_time": "2023-05-16T10:44:36.793597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The sky is crowing late one day and the summertime,  when the world is not to come home in my face is all the blues oh i know i can't find\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('The sky is') #Let's look at the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6353ecac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:44:39.159717Z",
     "iopub.status.busy": "2023-05-16T10:44:39.159336Z",
     "iopub.status.idle": "2023-05-16T10:44:40.667130Z",
     "shell.execute_reply": "2023-05-16T10:44:40.666032Z"
    },
    "papermill": {
     "duration": 1.852151,
     "end_time": "2023-05-16T10:44:40.669875",
     "exception": false,
     "start_time": "2023-05-16T10:44:38.817724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love has no one to the ground but if we gotta stay  that death is the end of the music you know the sands and he didn't i know\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('I love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bb08c92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:44:41.300031Z",
     "iopub.status.busy": "2023-05-16T10:44:41.299599Z",
     "iopub.status.idle": "2023-05-16T10:44:42.773231Z",
     "shell.execute_reply": "2023-05-16T10:44:42.772019Z"
    },
    "papermill": {
     "duration": 1.791709,
     "end_time": "2023-05-16T10:44:42.775865",
     "exception": false,
     "start_time": "2023-05-16T10:44:40.984156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can i can't take your side road, he rose without a fawn i know that i never had a letter at all, and it's just like to you can hardly hear\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('Can i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bcd143f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:44:43.416546Z",
     "iopub.status.busy": "2023-05-16T10:44:43.416163Z",
     "iopub.status.idle": "2023-05-16T10:44:43.421241Z",
     "shell.execute_reply": "2023-05-16T10:44:43.420134Z"
    },
    "papermill": {
     "duration": 0.319601,
     "end_time": "2023-05-16T10:44:43.424301",
     "exception": false,
     "start_time": "2023-05-16T10:44:43.104700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#These are bot scripts - probably the simplest of all that you have seen.\n",
    "\n",
    "#bot = telebot.TeleBot(\"API\")\n",
    "\n",
    "#@bot.message_handler(commands=['start'])\n",
    "#def start_message(message):\n",
    "    #bot.send_message(message.chat.id,'Write some words!')\n",
    "    \n",
    "\n",
    "#@bot.message_handler(content_types=['text'])\n",
    "#def after_text(message):\n",
    "    #j = message.text\n",
    "    #bot.send_message(message.chat.id, generate_text(j))\n",
    "    \n",
    "#bot.infinity_polling()\n",
    "\n",
    "#Where should I put the references? This link should definitely be here, a lot is taken from there\n",
    "#https://stackabuse.com/python-for-nlp-deep-learning-text-generation-with-keras/\n",
    "#Write to me in Telegram if you are interested in the project @semyonandreev."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 397.60542,
   "end_time": "2023-05-16T10:44:46.839840",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-16T10:38:09.234420",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
